<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive DQN Visualization</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>
</head>
<body class="bg-white dark:bg-gray-900 text-gray-900 dark:text-gray-100 transition-colors duration-200">
    <body class="bg-white dark:bg-gray-900 text-gray-900 dark:text-gray-100 transition-colors duration-200">
        <a href="../algorithms.html" style="position: fixed; top: 20px; left: 20px; z-index: 1000; background: #bbbbbb; color: #000000; border: 1px solid rgba(52, 152, 219, 0.2); border-radius: 5px; padding: 8px 16px; font-family: 'Roboto Mono', monospace; font-size: 14px; font-weight: 500; cursor: pointer; text-decoration: none; display: flex; align-items: center; gap: 8px; transition: all 0.3s ease;">
            <span style="font-size: 18px;">←</span> Return to Algorithms
        </a>
    <div class="container mx-auto px-4 py-8">
        <h1 class="text-3xl font-bold mb-6 text-center text-[#3498db] dark:text-[#3498db]">Deep Q-Network (DQN) Reinforcement Learning</h1>
        
        <div class="flex flex-col lg:flex-row gap-6">
            <!-- Environment and Network Visualization Container -->
            <div class="w-full lg:w-3/5 space-y-6">
                <!-- Grid World Environment -->
                <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4">
                    <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db]">Grid World Environment</h2>
                    <div class="w-full h-[400px] flex justify-center items-center relative" id="grid-world-container">
                        <canvas id="grid-world-canvas" width="400" height="400" class="border border-gray-300 dark:border-gray-600"></canvas>
                    </div>
                    <div class="mt-2 text-sm text-center">
                        <span>Agent learning to navigate through the environment to reach a goal</span>
                    </div>
                    
                    <!-- Environment Legend -->
                    <div class="mt-4 flex flex-wrap justify-center gap-4">
                        <div class="flex items-center">
                            <div class="w-5 h-5 bg-blue-500 rounded-sm mr-2"></div>
                            <span>Agent</span>
                        </div>
                        <div class="flex items-center">
                            <div class="w-5 h-5 bg-green-500 rounded-sm mr-2"></div>
                            <span>Goal</span>
                        </div>
                        <div class="flex items-center">
                            <div class="w-5 h-5 bg-red-500 rounded-sm mr-2"></div>
                            <span>Trap</span>
                        </div>
                        <div class="flex items-center">
                            <div class="w-5 h-5 bg-gray-700 dark:bg-gray-500 rounded-sm mr-2"></div>
                            <span>Wall</span>
                        </div>
                        <div class="flex items-center">
                            <div class="w-5 h-5 bg-gradient-to-r from-yellow-100 to-yellow-400 rounded-sm mr-2"></div>
                            <span>Q-Value Intensity</span>
                        </div>
                    </div>
                </div>
                
                <!-- Neural Network Visualization -->
                <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4">
                    <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db]">Neural Network Model</h2>
                    <div class="w-full h-[250px] relative" id="network-container">
                        <svg id="network-svg" class="w-full h-full"></svg>
                    </div>
                    <div class="mt-2 text-sm text-center">
                        <span>DQN model architecture and activations</span>
                    </div>
                </div>
                
                <!-- Multi-Chart Display -->
                <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4">
                    <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db]">Training Metrics</h2>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                        <div>
                            <h3 class="text-base font-medium mb-2 text-center">Reward & Steps</h3>
                            <div class="w-full h-[200px]">
                                <canvas id="learning-chart" class="w-full h-full"></canvas>
                            </div>
                        </div>
                        <div>
                            <h3 class="text-base font-medium mb-2 text-center">Loss & Epsilon</h3>
                            <div class="w-full h-[200px]">
                                <canvas id="loss-chart" class="w-full h-full"></canvas>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Controls Panel and Model Information -->
            <div class="w-full lg:w-2/5">
                <!-- Controls Panel -->
                <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4 mb-6">
                    <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db]">Controls</h2>
                    
                    <div class="space-y-6">
                        <!-- Environment Setup -->
                        <div>
                            <h3 class="text-lg font-medium mb-2">Environment Setup</h3>
                            <div class="flex flex-col gap-2">
                                <div class="flex items-center justify-between">
                                    <label for="grid-size" class="mr-2">Grid Size:</label>
                                    <select id="grid-size" class="w-20 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                        <option value="4">4×4</option>
                                        <option value="5" selected>5×5</option>
                                        <option value="6">6×6</option>
                                        <option value="8">8×8</option>
                                        <option value="10">10×10</option>
                                    </select>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="environment-type" class="mr-2">Environment Type:</label>
                                    <select id="environment-type" class="w-40 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                        <option value="empty" selected>Empty Room</option>
                                        <option value="maze">Simple Maze</option>
                                        <option value="obstacles">Random Obstacles</option>
                                        <option value="cliff">Cliff Walking</option>
                                        <option value="ice">Slippery Ice (Stochastic)</option>
                                    </select>
                                </div>
                                
                                <div class="flex items-center gap-2">
                                    <input type="checkbox" id="edit-mode" class="w-4 h-4">
                                    <label for="edit-mode">Edit Environment</label>
                                </div>
                                
                                <div class="grid grid-cols-2 gap-2">
                                    <div class="edit-tool-container flex items-center gap-2">
                                        <input type="radio" name="edit-tool" id="wall-tool" value="wall" class="w-4 h-4" checked>
                                        <label for="wall-tool">Wall</label>
                                    </div>
                                    <div class="edit-tool-container flex items-center gap-2">
                                        <input type="radio" name="edit-tool" id="start-tool" value="start" class="w-4 h-4">
                                        <label for="start-tool">Start</label>
                                    </div>
                                    <div class="edit-tool-container flex items-center gap-2">
                                        <input type="radio" name="edit-tool" id="goal-tool" value="goal" class="w-4 h-4">
                                        <label for="goal-tool">Goal</label>
                                    </div>
                                    <div class="edit-tool-container flex items-center gap-2">
                                        <input type="radio" name="edit-tool" id="trap-tool" value="trap" class="w-4 h-4">
                                        <label for="trap-tool">Trap</label>
                                    </div>
                                    <div class="edit-tool-container flex items-center gap-2">
                                        <input type="radio" name="edit-tool" id="erase-tool" value="erase" class="w-4 h-4">
                                        <label for="erase-tool">Erase</label>
                                    </div>
                                </div>
                                
                                <button id="reset-environment" class="bg-[#3498db] hover:bg-[#4A4ACB] text-white font-medium py-2 px-4 rounded transition">Reset Environment</button>
                            </div>
                        </div>
                        
                        <!-- DQN Parameters -->
                        <div>
                            <h3 class="text-lg font-medium mb-2">DQN Parameters</h3>
                            <div class="flex flex-col gap-2">
                                <!-- Network Architecture -->
                                <div class="flex items-center justify-between">
                                    <label for="network-architecture" class="mr-2">Network Architecture:</label>
                                    <select id="network-architecture" class="w-40 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                        <option value="small">Small (64,32)</option>
                                        <option value="medium" selected>Medium (128,64)</option>
                                        <option value="large">Large (256,128,64)</option>
                                    </select>
                                </div>
                                
                                <!-- Hyperparameters -->
                                <div class="flex items-center justify-between">
                                    <label for="learning-rate" class="mr-2">Learning Rate:</label>
                                    <div class="flex items-center">
                                        <input type="range" id="learning-rate" min="0.0001" max="0.01" step="0.0001" value="0.001" class="w-32">
                                        <span id="learning-rate-value" class="ml-2 w-16 text-sm">0.001</span>
                                    </div>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="discount-factor" class="mr-2">Discount Factor (γ):</label>
                                    <div class="flex items-center">
                                        <input type="range" id="discount-factor" min="0.8" max="0.999" step="0.001" value="0.95" class="w-32">
                                        <span id="discount-factor-value" class="ml-2 w-12 text-sm">0.95</span>
                                    </div>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="epsilon-start" class="mr-2">Initial Epsilon:</label>
                                    <div class="flex items-center">
                                        <input type="range" id="epsilon-start" min="0.1" max="1" step="0.05" value="1" class="w-32">
                                        <span id="epsilon-start-value" class="ml-2 w-12 text-sm">1.00</span>
                                    </div>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="epsilon-end" class="mr-2">Final Epsilon:</label>
                                    <div class="flex items-center">
                                        <input type="range" id="epsilon-end" min="0.01" max="0.5" step="0.01" value="0.1" class="w-32">
                                        <span id="epsilon-end-value" class="ml-2 w-12 text-sm">0.10</span>
                                    </div>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="epsilon-decay" class="mr-2">Epsilon Decay:</label>
                                    <div class="flex items-center">
                                        <input type="range" id="epsilon-decay" min="100" max="10000" step="100" value="1000" class="w-32">
                                        <span id="epsilon-decay-value" class="ml-2 w-16 text-sm">1000</span>
                                    </div>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="batch-size" class="mr-2">Batch Size:</label>
                                    <select id="batch-size" class="w-20 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                        <option value="16">16</option>
                                        <option value="32" selected>32</option>
                                        <option value="64">64</option>
                                        <option value="128">128</option>
                                    </select>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="replay-buffer-size" class="mr-2">Replay Buffer Size:</label>
                                    <select id="replay-buffer-size" class="w-24 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                        <option value="1000">1,000</option>
                                        <option value="5000">5,000</option>
                                        <option value="10000" selected>10,000</option>
                                        <option value="50000">50,000</option>
                                    </select>
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="target-update-freq" class="mr-2">Target Update Frequency:</label>
                                    <input type="number" id="target-update-freq" min="10" max="5000" value="500" class="w-24 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                </div>
                                
                                <div class="flex items-center justify-between">
                                    <label for="training-episodes" class="mr-2">Training Episodes:</label>
                                    <input type="number" id="training-episodes" min="10" max="5000" value="500" class="w-24 px-2 py-1 border rounded dark:bg-gray-700 dark:border-gray-600 dark:text-white text-base">
                                </div>
                                
                                <div class="flex space-x-2 mt-2">
                                    <button id="run-dqn" class="bg-[#3498db] hover:bg-[#4A4ACB] text-white font-medium py-2 px-4 rounded transition flex-1">Train</button>
                                    <button id="step-dqn" class="bg-[#3498db] hover:bg-[#4A4ACB] text-white font-medium py-2 px-4 rounded transition flex-1">Step</button>
                                    <button id="reset-dqn" class="bg-gray-500 hover:bg-gray-600 text-white font-medium py-2 px-4 rounded transition flex-1">Reset</button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Visualization Options -->
                        <div>
                            <h3 class="text-lg font-medium mb-2">Visualization Options</h3>
                            <div class="flex flex-col gap-2">
                                <div class="flex items-center gap-2">
                                    <input type="checkbox" id="show-q-values" class="w-4 h-4" checked>
                                    <label for="show-q-values">Show Q-Values</label>
                                </div>
                                <div class="flex items-center gap-2">
                                    <input type="checkbox" id="show-policy" class="w-4 h-4" checked>
                                    <label for="show-policy">Show Policy</label>
                                </div>
                                <div class="flex items-center gap-2">
                                    <input type="checkbox" id="show-network-details" class="w-4 h-4" checked>
                                    <label for="show-network-details">Show Network Details</label>
                                </div>
                                <div class="flex items-center gap-2">
                                    <input type="checkbox" id="show-replay-buffer" class="w-4 h-4">
                                    <label for="show-replay-buffer">Show Replay Buffer</label>
                                </div>
                                <div class="flex items-center justify-between">
                                    <label for="animation-speed" class="mr-2">Animation Speed:</label>
                                    <input type="range" id="animation-speed" min="1" max="100" step="1" value="50" class="w-32">
                                </div>
                                <div class="flex items-center gap-2 mt-2">
                                    <button id="run-agent" class="bg-green-500 hover:bg-green-600 text-white font-medium py-2 px-4 rounded transition flex-1">Run Agent</button>
                                    <button id="step-agent" class="bg-green-500 hover:bg-green-600 text-white font-medium py-2 px-4 rounded transition flex-1">Step Agent</button>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Algorithm Status -->
                        <div id="algorithm-status" class="p-3 bg-gray-100 dark:bg-gray-700 rounded">
                            <h3 class="text-lg font-medium mb-2">Status</h3>
                            <div class="space-y-2">
                                <div class="flex justify-between">
                                    <span>Episode:</span>
                                    <span id="current-episode">0/0</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Step:</span>
                                    <span id="current-step">0</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Current Epsilon:</span>
                                    <span id="current-epsilon">1.00</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Replay Buffer:</span>
                                    <span id="buffer-size">0/10000</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Loss:</span>
                                    <span id="current-loss">-</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Total Reward:</span>
                                    <span id="total-reward">0</span>
                                </div>
                                <div class="flex justify-between">
                                    <span>Status:</span>
                                    <span id="algorithm-status-text">Not Started</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Model Information Panel -->
                <div class="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4 mb-6">
                    <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db] flex justify-between items-center">
                        <span>Model Information</span>
                        <button id="toggle-model-info" class="text-sm px-2 py-1 bg-gray-200 dark:bg-gray-700 rounded">
                            Show Details
                        </button>
                    </h2>
                    <div id="model-info-container" class="hidden">
                        <div class="space-y-4">
                            <div>
                                <h3 class="text-base font-medium mb-1">Model Summary</h3>
                                <div id="model-summary" class="bg-gray-100 dark:bg-gray-700 p-3 rounded text-sm font-mono overflow-x-auto whitespace-pre"></div>
                            </div>
                            <div>
                                <h3 class="text-base font-medium mb-1">Input Shape</h3>
                                <div id="input-shape" class="bg-gray-100 dark:bg-gray-700 p-3 rounded"></div>
                            </div>
                            <div>
                                <h3 class="text-base font-medium mb-1">Output Shape</h3>
                                <div id="output-shape" class="bg-gray-100 dark:bg-gray-700 p-3 rounded"></div>
                            </div>
                            <div>
                                <h3 class="text-base font-medium mb-1">Experience Replay</h3>
                                <div id="experience-replay-info" class="bg-gray-100 dark:bg-gray-700 p-3 rounded"></div>
                            </div>
                        </div>
                    </div>
                    <div id="replay-buffer-visualization" class="mt-4 hidden">
                        <h3 class="text-base font-medium mb-2">Replay Buffer Samples</h3>
                        <div class="grid grid-cols-4 gap-1 text-xs">
                            <div class="font-semibold">State</div>
                            <div class="font-semibold">Action</div>
                            <div class="font-semibold">Reward</div>
                            <div class="font-semibold">Next State</div>
                        </div>
                        <div id="replay-samples" class="max-h-40 overflow-y-auto"></div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Explanation Section -->
        <div class="mt-6 bg-white dark:bg-gray-800 rounded-lg shadow-lg p-4">
            <h2 class="text-xl font-semibold mb-4 text-[#3498db] dark:text-[#3498db]">How Deep Q-Networks (DQN) Work</h2>
            <div class="space-y-3 text-sm md:text-base">
                <p><strong>Deep Q-Network (DQN)</strong> is a reinforcement learning algorithm that combines Q-learning with deep neural networks to handle high-dimensional state spaces. It was introduced by DeepMind in 2015 and represents a breakthrough in reinforcement learning, enabling agents to learn directly from raw sensory inputs.</p>
                
                <p><strong>Key Components of DQN:</strong></p>
                <ol class="list-decimal ml-6">
                    <li><strong>Neural Network as Function Approximator:</strong> Instead of maintaining a Q-table, DQN uses a neural network to approximate the Q-function, allowing it to handle much larger state spaces.</li>
                    <li><strong>Experience Replay:</strong> DQN stores experiences (state, action, reward, next state) in a replay buffer and samples random batches for training, which breaks correlations between consecutive samples and improves data efficiency.</li>
                    <li><strong>Target Network:</strong> A separate "target" network is used for generating the targets in the Q-learning update, which is periodically updated with the weights of the main network to improve stability.</li>
                    <li><strong>Epsilon-Greedy Exploration:</strong> DQN starts with a high exploration rate (epsilon) that gradually decreases over time, balancing exploration and exploitation.</li>
                </ol>
                
                <p><strong>DQN Algorithm:</strong></p>
                <ol class="list-decimal ml-6">
                    <li>Initialize replay memory D to capacity N</li>
                    <li>Initialize action-value function Q with random weights θ</li>
                    <li>Initialize target action-value function Q̂ with weights θ⁻ = θ</li>
                    <li>For each episode:
                        <ul class="list-disc ml-6">
                            <li>Initialize state s₁</li>
                            <li>For each step of the episode:
                                <ul class="list-disc ml-6">
                                    <li>With probability ε select a random action aₜ, otherwise select aₜ = argmax_a Q(sₜ,a;θ)</li>
                                    <li>Execute action aₜ in the environment and observe reward rₜ and next state sₜ₊₁</li>
                                    <li>Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in replay memory D</li>
                                    <li>Sample random mini-batch of transitions from D</li>
                                    <li>Set y_j = rⱼ if episode terminates at step j+1, otherwise y_j = rⱼ + γ max_a' Q̂(sⱼ₊₁,a';θ⁻)</li>
                                    <li>Perform a gradient descent step on (y_j - Q(sⱼ,aⱼ;θ))² with respect to θ</li>
                                    <li>Every C steps update target network parameters: θ⁻ = θ</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ol>
                
                <p><strong>Improvements and Variations:</strong></p>
                <ul class="list-disc ml-6">
                    <li><strong>Double DQN:</strong> Addresses the overestimation bias in Q-learning by using the online network to select actions and the target network to evaluate them.</li>
                    <li><strong>Dueling DQN:</strong> Separates the value and advantage functions, allowing the network to learn which states are valuable without having to learn the effect of each action.</li>
                    <li><strong>Prioritized Experience Replay:</strong> Samples transitions with higher expected learning progress more frequently.</li>
                    <li><strong>Noisy DQN:</strong> Uses noisy linear layers for directed exploration instead of epsilon-greedy.</li>
                    <li><strong>Rainbow DQN:</strong> Combines multiple improvements for state-of-the-art performance.</li>
                </ul>
                
                <p><strong>Advantages over traditional Q-Learning:</strong></p>
                <ul class="list-disc ml-6">
                    <li>Can handle high-dimensional state spaces (e.g., pixels from a game screen)</li>
                    <li>Better generalization to unseen states through function approximation</li>
                    <li>Experience replay improves data efficiency and breaks correlations</li>
                    <li>More stable learning through target networks</li>
                    <li>Ability to learn complex strategies in challenging environments</li>
                </ul>
            </div>
        </div>
    </div>

    <script>
        // Initialize dark mode based on user preference
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            document.body.classList.add('dark');
        }
        
        // DOM elements
        const gridWorldCanvas = document.getElementById('grid-world-canvas');
        const networkSvg = document.getElementById('network-svg');
        const learningChart = document.getElementById('learning-chart');
        const lossChart = document.getElementById('loss-chart');
        const gridSizeSelect = document.getElementById('grid-size');
        const environmentTypeSelect = document.getElementById('environment-type');
        const editModeCheckbox = document.getElementById('edit-mode');
        const editTools = document.querySelectorAll('input[name="edit-tool"]');
        const resetEnvironmentBtn = document.getElementById('reset-environment');
        const networkArchitectureSelect = document.getElementById('network-architecture');
        const learningRateInput = document.getElementById('learning-rate');
        const learningRateValue = document.getElementById('learning-rate-value');
        const discountFactorInput = document.getElementById('discount-factor');
        const discountFactorValue = document.getElementById('discount-factor-value');
        const epsilonStartInput = document.getElementById('epsilon-start');
        const epsilonStartValue = document.getElementById('epsilon-start-value');
        const epsilonEndInput = document.getElementById('epsilon-end');
        const epsilonEndValue = document.getElementById('epsilon-end-value');
        const epsilonDecayInput = document.getElementById('epsilon-decay');
        const epsilonDecayValue = document.getElementById('epsilon-decay-value');
        const batchSizeSelect = document.getElementById('batch-size');
        const replayBufferSizeSelect = document.getElementById('replay-buffer-size');
        const targetUpdateFreqInput = document.getElementById('target-update-freq');
        const trainingEpisodesInput = document.getElementById('training-episodes');
        const runDqnBtn = document.getElementById('run-dqn');
        const stepDqnBtn = document.getElementById('step-dqn');
        const resetDqnBtn = document.getElementById('reset-dqn');
        const showQValuesCheckbox = document.getElementById('show-q-values');
        const showPolicyCheckbox = document.getElementById('show-policy');
        const showNetworkDetailsCheckbox = document.getElementById('show-network-details');
        const showReplayBufferCheckbox = document.getElementById('show-replay-buffer');
        const animationSpeedInput = document.getElementById('animation-speed');
        const runAgentBtn = document.getElementById('run-agent');
        const stepAgentBtn = document.getElementById('step-agent');
        const currentEpisodeDisplay = document.getElementById('current-episode');
        const currentStepDisplay = document.getElementById('current-step');
        const currentEpsilonDisplay = document.getElementById('current-epsilon');
        const bufferSizeDisplay = document.getElementById('buffer-size');
        const currentLossDisplay = document.getElementById('current-loss');
        const totalRewardDisplay = document.getElementById('total-reward');
        const algorithmStatusDisplay = document.getElementById('algorithm-status-text');
        const toggleModelInfoBtn = document.getElementById('toggle-model-info');
        const modelInfoContainer = document.getElementById('model-info-container');
        const modelSummaryDisplay = document.getElementById('model-summary');
        const inputShapeDisplay = document.getElementById('input-shape');
        const outputShapeDisplay = document.getElementById('output-shape');
        const experienceReplayInfoDisplay = document.getElementById('experience-replay-info');
        const replayBufferVisualization = document.getElementById('replay-buffer-visualization');
        const replaySamplesDisplay = document.getElementById('replay-samples');
        
        // Canvas context
        const ctx = gridWorldCanvas.getContext('2d');
        
        // Colors
        const COLORS = {
            empty: { light: '#f8fafc', dark: '#1F2937' },
            agent: { light: '#3B82F6', dark: '#3B82F6' }, // Blue
            goal: { light: '#10B981', dark: '#10B981' },  // Green
            trap: { light: '#EF4444', dark: '#EF4444' },  // Red
            wall: { light: '#4B5563', dark: '#9CA3AF' },  // Gray
            start: { light: '#8B5CF6', dark: '#8B5CF6' }, // Purple
            text: { light: '#1F2937', dark: '#F3F4F6' },
            arrow: { light: '#000000', dark: '#FFFFFF' },
            grid: { light: '#E2E8F0', dark: '#374151' },
            qvalue: { light: '#FBBF24', dark: '#FBBF24' }, // Amber
            neuron: { light: '#60A5FA', dark: '#60A5FA' }, // Blue
            activeNeuron: { light: '#F97316', dark: '#F97316' }, // Orange
            connection: { light: '#9CA3AF', dark: '#6B7280' }, // Gray
            activeConnection: { light: '#F59E0B', dark: '#F59E0B' } // Amber
        };
        
        // DQN parameters
        let learningRate = 0.001;       // Learning rate for neural network
        let discountFactor = 0.95;      // γ (gamma) discount factor
        let epsilonStart = 1.0;         // Initial exploration rate
        let epsilonEnd = 0.1;           // Final exploration rate
        let epsilonDecay = 1000;        // Epsilon decay rate
        let batchSize = 32;             // Batch size for training
        let replayBufferSize = 10000;   // Size of experience replay buffer
        let targetUpdateFrequency = 500; // Steps between target network updates
        let maxEpisodes = 500;          // Maximum training episodes
        
        // Environment and grid parameters
        let gridSize = 5;
        let cellSize = gridWorldCanvas.width / gridSize;
        let environment = [];
        let startPosition = { x: 0, y: 0 };
        let agentPosition = { x: 0, y: 0 };
        let goalPosition = { x: gridSize - 1, y: gridSize - 1 };
        let stateCount = gridSize * gridSize;
        let actionCount = 4; // Up, Right, Down, Left
        
        // DQN algorithm state
        let qNetwork = null;            // Main Q-network
        let targetNetwork = null;       // Target network for stable learning
        let replayBuffer = [];          // Experience replay buffer
        let optimizer = null;           // Optimizer for training
        let currentEpsilon = 1.0;       // Current exploration rate
        let currentEpisode = 0;         // Current episode
        let stepCount = 0;              // Current step count
        let totalReward = 0;            // Cumulative reward
        let episodeRewards = [];        // Rewards for each episode
        let episodeSteps = [];          // Steps for each episode
        let losses = [];                // Loss values during training
        let epsilonHistory = [];        // Epsilon values during training
        let isTraining = false;         // Flag for training state
        let isRunningAgent = false;     // Flag for running the trained agent
        let animationSpeed = 50;        // Animation speed for visualization
        let stepByStepMode = false;     // Flag for step-by-step execution
        let editMode = false;           // Flag for environment editing
        let currentTool = 'wall';       // Current editing tool
        let networkLayerSizes = [];     // Sizes of neural network layers
        let currentActivations = [];    // Current activations in the network
        let trainingSteps = 0;          // Total training steps
        let lastBatchLoss = 0;          // Loss from the most recent batch
        
        // Rewards
        const REWARDS = {
            step: -0.1,   // Small negative reward for each step
            goal: 1.0,    // Positive reward for reaching the goal
            trap: -1.0,   // Negative reward for falling into a trap
            wall: -0.3    // Negative reward for hitting a wall
        };
        
        // Action mapping (0: Up, 1: Right, 2: Down, 3: Left)
        const ACTIONS = [
            { dx: 0, dy: -1 }, // Up
            { dx: 1, dy: 0 },  // Right
            { dx: 0, dy: 1 },  // Down
            { dx: -1, dy: 0 }  // Left
        ];
        
        // Action names for display
        const ACTION_NAMES = ['Up', 'Right', 'Down', 'Left'];
        
        // Initialize learning chart
        let learningChartInstance;
        let lossChartInstance;
        
        // Initialize application
        function initializeApp() {
            // Enable TensorFlow.js
            tf.setBackend('webgl');
            
            // Set up the grid
            resetEnvironment();
            
            // Initialize DQN model
            resetDQN();
            
            // Initialize charts
            initializeCharts();
            
            // Set initial parameter values
            updateLearningRateDisplay();
            updateDiscountFactorDisplay();
            updateEpsilonStartDisplay();
            updateEpsilonEndDisplay();
            updateEpsilonDecayDisplay();
            
            // Add event listeners
            addEventListeners();
            
            // Render the initial state
            render();
            
            // Draw neural network visualization
            drawNetworkVisualization();
        }
        
        // Initialize charts
        function initializeCharts() {
            const isDarkMode = document.body.classList.contains('dark');
            const textColor = isDarkMode ? '#D1D5DB' : '#1F2937';
            const gridColor = isDarkMode ? 'rgba(255, 255, 255, 0.1)' : 'rgba(0, 0, 0, 0.1)';
            
            // Learning chart (Reward & Steps)
            const learningCtx = learningChart.getContext('2d');
            
            // Destroy existing chart if it exists
            if (learningChartInstance) {
                learningChartInstance.destroy();
            }
            
            learningChartInstance = new Chart(learningCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [
                        {
                            label: 'Reward',
                            data: [],
                            borderColor: '#3B82F6',
                            backgroundColor: 'rgba(59, 130, 246, 0.2)',
                            tension: 0.1,
                            yAxisID: 'y'
                        },
                        {
                            label: 'Steps',
                            data: [],
                            borderColor: '#EF4444',
                            backgroundColor: 'rgba(239, 68, 68, 0.2)',
                            tension: 0.1,
                            yAxisID: 'y1'
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    interaction: {
                        mode: 'index',
                        intersect: false,
                    },
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Episode',
                                color: textColor
                            },
                            grid: {
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Reward',
                                color: textColor
                            },
                            position: 'left',
                            grid: {
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        },
                        y1: {
                            title: {
                                display: true,
                                text: 'Steps',
                                color: textColor
                            },
                            position: 'right',
                            grid: {
                                drawOnChartArea: false,
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        }
                    },
                    plugins: {
                        legend: {
                            labels: {
                                color: textColor
                            }
                        }
                    }
                }
            });
            
            // Loss chart (Loss & Epsilon)
            const lossCtx = lossChart.getContext('2d');
            
            // Destroy existing chart if it exists
            if (lossChartInstance) {
                lossChartInstance.destroy();
            }
            
            lossChartInstance = new Chart(lossCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [
                        {
                            label: 'Loss',
                            data: [],
                            borderColor: '#F59E0B',
                            backgroundColor: 'rgba(245, 158, 11, 0.2)',
                            tension: 0.1,
                            yAxisID: 'y'
                        },
                        {
                            label: 'Epsilon',
                            data: [],
                            borderColor: '#8B5CF6',
                            backgroundColor: 'rgba(139, 92, 246, 0.2)',
                            tension: 0.1,
                            yAxisID: 'y1'
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    interaction: {
                        mode: 'index',
                        intersect: false,
                    },
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Training Step',
                                color: textColor
                            },
                            grid: {
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Loss',
                                color: textColor
                            },
                            position: 'left',
                            grid: {
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        },
                        y1: {
                            title: {
                                display: true,
                                text: 'Epsilon',
                                color: textColor
                            },
                            position: 'right',
                            min: 0,
                            max: 1,
                            grid: {
                                drawOnChartArea: false,
                                color: gridColor
                            },
                            ticks: {
                                color: textColor
                            }
                        }
                    },
                    plugins: {
                        legend: {
                            labels: {
                                color: textColor
                            }
                        }
                    }
                }
            });
        }
        
        // Add event listeners
        function addEventListeners() {
            // Grid size change
            gridSizeSelect.addEventListener('change', () => {
                gridSize = parseInt(gridSizeSelect.value);
                cellSize = gridWorldCanvas.width / gridSize;
                stateCount = gridSize * gridSize;
                resetEnvironment();
                resetDQN();
                render();
                drawNetworkVisualization();
            });
            
            // Environment type change
            environmentTypeSelect.addEventListener('change', () => {
                resetEnvironment();
                render();
            });
            
            // Edit mode toggle
            editModeCheckbox.addEventListener('change', () => {
                editMode = editModeCheckbox.checked;
                
                // Enable/disable tool options
                document.querySelectorAll('.edit-tool-container').forEach(container => {
                    const input = container.querySelector('input');
                    input.disabled = !editMode;
                    if (editMode) {
                        container.classList.remove('opacity-50');
                    } else {
                        container.classList.add('opacity-50');
                    }
                });
                
                render();
            });
            
            // Edit tools
            editTools.forEach(tool => {
                tool.addEventListener('change', () => {
                    currentTool = tool.value;
                });
            });
            
            // Reset environment
            resetEnvironmentBtn.addEventListener('click', () => {
                resetEnvironment();
                render();
            });
            
            // Network architecture change
            networkArchitectureSelect.addEventListener('change', () => {
                resetDQN();
                drawNetworkVisualization();
            });
            
            // Parameter changes
            learningRateInput.addEventListener('input', () => {
                learningRate = parseFloat(learningRateInput.value);
                updateLearningRateDisplay();
                resetDQN();
            });
            
            discountFactorInput.addEventListener('input', () => {
                discountFactor = parseFloat(discountFactorInput.value);
                updateDiscountFactorDisplay();
            });
            
            epsilonStartInput.addEventListener('input', () => {
                epsilonStart = parseFloat(epsilonStartInput.value);
                updateEpsilonStartDisplay();
                // Ensure epsilonStart >= epsilonEnd
                if (epsilonStart < epsilonEnd) {
                    epsilonEnd = epsilonStart;
                    epsilonEndInput.value = epsilonEnd;
                    updateEpsilonEndDisplay();
                }
                currentEpsilon = epsilonStart;
                currentEpsilonDisplay.textContent = currentEpsilon.toFixed(2);
            });
            
            epsilonEndInput.addEventListener('input', () => {
                epsilonEnd = parseFloat(epsilonEndInput.value);
                updateEpsilonEndDisplay();
                // Ensure epsilonEnd <= epsilonStart
                if (epsilonEnd > epsilonStart) {
                    epsilonStart = epsilonEnd;
                    epsilonStartInput.value = epsilonStart;
                    updateEpsilonStartDisplay();
                }
            });
            
            epsilonDecayInput.addEventListener('input', () => {
                epsilonDecay = parseInt(epsilonDecayInput.value);
                updateEpsilonDecayDisplay();
            });
            
            batchSizeSelect.addEventListener('change', () => {
                batchSize = parseInt(batchSizeSelect.value);
            });
            
            replayBufferSizeSelect.addEventListener('change', () => {
                replayBufferSize = parseInt(replayBufferSizeSelect.value);
                bufferSizeDisplay.textContent = `0/${replayBufferSize}`;
                replayBuffer = [];
            });
            
            targetUpdateFreqInput.addEventListener('change', () => {
                targetUpdateFrequency = parseInt(targetUpdateFreqInput.value);
            });
            
            trainingEpisodesInput.addEventListener('change', () => {
                maxEpisodes = parseInt(trainingEpisodesInput.value);
            });
            
            // Animation speed
            animationSpeedInput.addEventListener('input', () => {
                animationSpeed = parseInt(animationSpeedInput.value);
            });
            
            // DQN controls
            runDqnBtn.addEventListener('click', runDQN);
            stepDqnBtn.addEventListener('click', stepDQN);
            resetDqnBtn.addEventListener('click', () => {
                resetDQN();
                render();
                drawNetworkVisualization();
                updateCharts();
            });
            
            // Agent controls
            runAgentBtn.addEventListener('click', runAgent);
            stepAgentBtn.addEventListener('click', stepAgent);
            
            // Visualization options
            showQValuesCheckbox.addEventListener('change', render);
            showPolicyCheckbox.addEventListener('change', render);
            showNetworkDetailsCheckbox.addEventListener('change', drawNetworkVisualization);
            showReplayBufferCheckbox.addEventListener('change', () => {
                if (showReplayBufferCheckbox.checked) {
                    replayBufferVisualization.classList.remove('hidden');
                    updateReplayBufferVisualization();
                } else {
                    replayBufferVisualization.classList.add('hidden');
                }
            });
            
            // Toggle model info
            toggleModelInfoBtn.addEventListener('click', () => {
                const isHidden = modelInfoContainer.classList.contains('hidden');
                if (isHidden) {
                    modelInfoContainer.classList.remove('hidden');
                    toggleModelInfoBtn.textContent = 'Hide Details';
                    updateModelInfo();
                } else {
                    modelInfoContainer.classList.add('hidden');
                    toggleModelInfoBtn.textContent = 'Show Details';
                }
            });
            
            // Canvas click event for editing
            gridWorldCanvas.addEventListener('click', handleCanvasClick);
        }
        
        // Handle canvas click for editing
        function handleCanvasClick(event) {
            if (!editMode) return;
            
            const rect = gridWorldCanvas.getBoundingClientRect();
            const x = event.clientX - rect.left;
            const y = event.clientY - rect.top;
            
            // Convert to grid coordinates
            const gridX = Math.floor(x / cellSize);
            const gridY = Math.floor(y / cellSize);
            
            // Check if within bounds
            if (gridX < 0 || gridX >= gridSize || gridY < 0 || gridY >= gridSize) {
                return;
            }
            
            // Apply the current tool
            switch (currentTool) {
                case 'wall':
                    environment[gridY][gridX] = 'wall';
                    break;
                case 'start':
                    // Remove old start position
                    environment[startPosition.y][startPosition.x] = 'empty';
                    // Set new start position
                    startPosition = { x: gridX, y: gridY };
                    agentPosition = { x: gridX, y: gridY };
                    environment[gridY][gridX] = 'start';
                    break;
                case 'goal':
                    // Remove old goal position
                    environment[goalPosition.y][goalPosition.x] = 'empty';
                    // Set new goal position
                    goalPosition = { x: gridX, y: gridY };
                    environment[gridY][gridX] = 'goal';
                    break;
                case 'trap':
                    environment[gridY][gridX] = 'trap';
                    break;
                case 'erase':
                    // Don't erase start or goal
                    if ((gridX === startPosition.x && gridY === startPosition.y) || 
                        (gridX === goalPosition.x && gridY === goalPosition.y)) {
                        return;
                    }
                    environment[gridY][gridX] = 'empty';
                    break;
            }
            
            // Redraw
            render();
            
            // Ensure path exists
            ensurePathExists();
        }
        
        // Reset the environment
        function resetEnvironment() {
            // Initialize the grid with empty cells
            environment = Array(gridSize).fill().map(() => Array(gridSize).fill('empty'));
            
            // Set up environment based on selected type
            const envType = environmentTypeSelect.value;
            
            switch (envType) {
                case 'empty':
                    // Just an empty room with start and goal
                    startPosition = { x: 0, y: 0 };
                    goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                    break;
                    
                case 'maze':
                    // Simple maze with some walls
                    startPosition = { x: 0, y: 0 };
                    goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                    
                    // Add some walls to form a simple maze
                    for (let i = 1; i < gridSize - 1; i += 2) {
                        for (let j = 0; j < gridSize; j++) {
                            // Skip some positions to create paths
                            if (Math.random() > 0.3) {
                                environment[i][j] = 'wall';
                            }
                        }
                    }
                    
                    // Ensure start and goal are accessible
                    environment[startPosition.y][startPosition.x] = 'empty';
                    environment[goalPosition.y][goalPosition.x] = 'empty';
                    break;
                    
                case 'obstacles':
                    // Random obstacles
                    startPosition = { x: 0, y: 0 };
                    goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                    
                    // Add random walls and traps
                    for (let i = 0; i < gridSize; i++) {
                        for (let j = 0; j < gridSize; j++) {
                            // Skip start and goal positions
                            if ((i === startPosition.y && j === startPosition.x) || 
                                (i === goalPosition.y && j === goalPosition.x)) {
                                continue;
                            }
                            
                            const rand = Math.random();
                            if (rand < 0.2) {
                                environment[i][j] = 'wall';
                            } else if (rand < 0.25) {
                                environment[i][j] = 'trap';
                            }
                        }
                    }
                    break;
                    
                case 'cliff':
                    // Cliff walking problem
                    startPosition = { x: 0, y: gridSize - 1 };
                    goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                    
                    // Create a "cliff" (row of traps) along the bottom
                    for (let j = 1; j < gridSize - 1; j++) {
                        environment[gridSize - 1][j] = 'trap';
                    }
                    break;
                    
                case 'ice':
                    // Slippery ice environment (implemented with stochastic transitions)
                    startPosition = { x: Math.floor(gridSize / 2), y: Math.floor(gridSize / 2) };
                    goalPosition = { x: gridSize - 1, y: gridSize - 1 };
                    
                    // Add some obstacles around the edges
                    for (let i = 0; i < gridSize; i++) {
                        if (i !== Math.floor(gridSize / 2)) {
                            environment[0][i] = 'wall';
                            environment[gridSize - 1][i] = 'wall';
                            environment[i][0] = 'wall';
                            environment[i][gridSize - 1] = 'wall';
                        }
                    }
                    
                    // Make sure goal is accessible
                    environment[goalPosition.y][goalPosition.x] = 'empty';
                    break;
            }
            
            // Set start and goal in the environment
            environment[startPosition.y][startPosition.x] = 'start';
            environment[goalPosition.y][goalPosition.x] = 'goal';
            
            // Reset agent position to start
            agentPosition = { ...startPosition };
            
            // Ensure path exists between start and goal
            ensurePathExists();
        }
        
        // Make sure there is at least one valid path from start to goal
        function ensurePathExists() {
            // Create a temporary copy of the environment to test path
            const tempEnv = environment.map(row => [...row]);
            
            // Breadth-first search to find a path
            const queue = [{ x: startPosition.x, y: startPosition.y, path: [] }];
            const visited = new Set();
            visited.add(`${startPosition.x},${startPosition.y}`);
            
            let pathFound = false;
            
            while (queue.length > 0 && !pathFound) {
                const { x, y, path } = queue.shift();
                
                // Check if reached goal
                if (x === goalPosition.x && y === goalPosition.y) {
                    pathFound = true;
                    break;
                }
                
                // Try all four directions
                for (const { dx, dy } of ACTIONS) {
                    const newX = x + dx;
                    const newY = y + dy;
                    const key = `${newX},${newY}`;
                    
                    // Skip if out of bounds
                    if (newX < 0 || newX >= gridSize || newY < 0 || newY >= gridSize) {
                        continue;
                    }
                    
                    // Skip if visited
                    if (visited.has(key)) {
                        continue;
                    }
                    
                    // Skip walls but allow traps (since they're still traversable)
                    if (tempEnv[newY][newX] === 'wall') {
                        continue;
                    }
                    
                    // Add to queue
                    visited.add(key);
                    queue.push({ x: newX, y: newY, path: [...path, { x: newX, y: newY }] });
                }
            }
            
            // If no path found, clear obstacles to create a path
            if (!pathFound) {
                // Reset obstacles and create a simple path from start to goal
                createSimplePath();
                render(); // Redraw the environment
            }
        }
        
        // Create a simple path from start to goal by clearing obstacles
        function createSimplePath() {
            // Create a simple L-shaped path from start to goal
            let currentX = startPosition.x;
            let currentY = startPosition.y;
            
            // Move horizontally to align with goal column
            while (currentX !== goalPosition.x) {
                currentX += (goalPosition.x > currentX) ? 1 : -1;
                
                // Clear any obstacles
                if (environment[currentY][currentX] === 'wall' || environment[currentY][currentX] === 'trap') {
                    environment[currentY][currentX] = 'empty';
                }
            }
            
            // Move vertically to reach goal
            while (currentY !== goalPosition.y) {
                currentY += (goalPosition.y > currentY) ? 1 : -1;
                
                // Clear any obstacles
                if (environment[currentY][currentX] === 'wall' || environment[currentY][currentX] === 'trap') {
                    environment[currentY][currentX] = 'empty';
                }
            }
        }
        
        // Reset DQN model and related state
        function resetDQN() {
            // Clear old models
            if (qNetwork) {
                qNetwork.dispose();
            }
            if (targetNetwork) {
                targetNetwork.dispose();
            }
            
            // Set up network architecture based on selection
            switch (networkArchitectureSelect.value) {
                case 'small':
                    networkLayerSizes = [64, 32];
                    break;
                case 'medium':
                    networkLayerSizes = [128, 64];
                    break;
                case 'large':
                    networkLayerSizes = [256, 128, 64];
                    break;
                default:
                    networkLayerSizes = [128, 64];
            }
            
            // Create Q-network and target network
            qNetwork = createNetwork();
            targetNetwork = createNetwork();
            
            // Synchronize target network
            updateTargetNetwork();
            
            // Create optimizer
            optimizer = tf.train.adam(learningRate);
            
            // Reset counters and buffers
            replayBuffer = [];
            currentEpsilon = epsilonStart;
            currentEpisode = 0;
            stepCount = 0;
            totalReward = 0;
            episodeRewards = [];
            episodeSteps = [];
            losses = [];
            epsilonHistory = [];
            trainingSteps = 0;
            lastBatchLoss = 0;
            
            // Reset agent position
            agentPosition = { ...startPosition };
            
            // Reset activations
            currentActivations = Array(networkLayerSizes.length + 1).fill().map(() => Array(0));
            
            // Reset UI displays
            currentEpisodeDisplay.textContent = `${currentEpisode}/${maxEpisodes}`;
            currentStepDisplay.textContent = stepCount;
            currentEpsilonDisplay.textContent = currentEpsilon.toFixed(2);
            bufferSizeDisplay.textContent = `0/${replayBufferSize}`;
            currentLossDisplay.textContent = '-';
            totalRewardDisplay.textContent = totalReward.toFixed(2);
            algorithmStatusDisplay.textContent = 'Reset';
            
            // Clear training flags
            isTraining = false;
            isRunningAgent = false;
            stepByStepMode = false;
            
            // Update model info
            updateModelInfo();
            
            // Reset visualization
            updateCharts();
        }
        
        // Create neural network model
        function createNetwork() {
            // Input layer: state (flattened one-hot encoding of position)
            const model = tf.sequential();
            
            // Input shape: one-hot encoding of grid position
            const inputShape = [stateCount];
            
            // Add layers
            model.add(tf.layers.dense({
                units: networkLayerSizes[0],
                activation: 'relu',
                inputShape
            }));
            
            // Add hidden layers
            for (let i = 1; i < networkLayerSizes.length; i++) {
                model.add(tf.layers.dense({
                    units: networkLayerSizes[i],
                    activation: 'relu'
                }));
            }
            
            // Output layer: Q-values for each action
            model.add(tf.layers.dense({
                units: actionCount,
                activation: 'linear'
            }));
            
            // Compile model
            model.compile({
                optimizer: tf.train.adam(learningRate),
                loss: 'meanSquaredError'
            });
            
            return model;
        }
        
        // Update target network weights
        function updateTargetNetwork() {
            if (!qNetwork || !targetNetwork) return;
            
            const qWeights = qNetwork.getWeights();
            targetNetwork.setWeights(qWeights);
        }
        
        // Update learning rate display
        function updateLearningRateDisplay() {
            learningRateValue.textContent = learningRate.toFixed(4);
        }
        
        // Update discount factor display
        function updateDiscountFactorDisplay() {
            discountFactorValue.textContent = discountFactor.toFixed(2);
        }
        
        // Update epsilon start display
        function updateEpsilonStartDisplay() {
            epsilonStartValue.textContent = epsilonStart.toFixed(2);
        }
        
        // Update epsilon end display
        function updateEpsilonEndDisplay() {
            epsilonEndValue.textContent = epsilonEnd.toFixed(2);
        }
        
        // Update epsilon decay display
        function updateEpsilonDecayDisplay() {
            epsilonDecayValue.textContent = epsilonDecay;
        }
        
        // Convert (x,y) coordinates to state index
        function coordsToState(x, y) {
            return y * gridSize + x;
        }
        
        // Convert state index to (x,y) coordinates
        function stateToCoords(state) {
            const x = state % gridSize;
            const y = Math.floor(state / gridSize);
            return { x, y };
        }
        
        // Create state representation (one-hot encoded)
        function createStateRepresentation(x, y) {
            const state = coordsToState(x, y);
            const stateVector = tf.buffer([stateCount]);
            stateVector.set(1, state);
            return stateVector.toTensor();
        }
        
        // Get the type of cell at given coordinates
        function getCellType(x, y) {
            // Check if out of bounds
            if (x < 0 || x >= gridSize || y < 0 || y >= gridSize) {
                return 'wall'; // Treat out of bounds as walls
            }
            return environment[y][x];
        }
        
        // Check if state is terminal (goal or trap)
        function isTerminalState(x, y) {
            const cellType = getCellType(x, y);
            return cellType === 'goal' || cellType === 'trap';
        }
        
        // Get possible actions from a state (removes actions that lead to walls)
        function getPossibleActions(x, y) {
            const possibleActions = [];
            
            for (let action = 0; action < actionCount; action++) {
                const { dx, dy } = ACTIONS[action];
                const newX = x + dx;
                const newY = y + dy;
                
                // Check if the new position is valid (not a wall and not out of bounds)
                if (getCellType(newX, newY) !== 'wall') {
                    possibleActions.push(action);
                }
            }
            
            return possibleActions;
        }
        
        // Choose an action using epsilon-greedy policy
        async function chooseAction(x, y) {
            // With probability epsilon, choose a random action
            if (Math.random() < currentEpsilon) {
                const possibleActions = getPossibleActions(x, y);
                return possibleActions[Math.floor(Math.random() * possibleActions.length)];
            }
            
            // Otherwise, choose the best action according to the Q-network
            const stateTensor = createStateRepresentation(x, y);
            
            // Get Q-values from network
            const qValues = await tf.tidy(() => {
                const predictions = qNetwork.predict(stateTensor.expandDims(0));
                return predictions.dataSync();
            });
            
            // Clean up tensors
            stateTensor.dispose();
            
            // Find best action (that doesn't lead to a wall)
            const possibleActions = getPossibleActions(x, y);
            
            // If no valid actions, return a random one (shouldn't happen in a well-formed grid)
            if (possibleActions.length === 0) {
                return Math.floor(Math.random() * actionCount);
            }
            
            // Find the action with highest Q-value among possible actions
            let bestAction = possibleActions[0];
            let bestValue = qValues[bestAction];
            
            for (let i = 1; i < possibleActions.length; i++) {
                const action = possibleActions[i];
                if (qValues[action] > bestValue) {
                    bestValue = qValues[action];
                    bestAction = action;
                }
            }
            
            // Store current activations for visualization
            await getNetworkActivations(x, y);
            
            return bestAction;
        }
        
        // Get best action from Q-network (no exploration)
        async function getBestAction(x, y) {
            const stateTensor = createStateRepresentation(x, y);
            
            // Get Q-values from network
            const qValues = await tf.tidy(() => {
                const predictions = qNetwork.predict(stateTensor.expandDims(0));
                return predictions.dataSync();
            });
            
            // Clean up tensors
            stateTensor.dispose();
            
            // Find best action (that doesn't lead to a wall)
            const possibleActions = getPossibleActions(x, y);
            
            // If no valid actions, return null (shouldn't happen in a well-formed grid)
            if (possibleActions.length === 0) {
                return null;
            }
            
            // Find the action with highest Q-value among possible actions
            let bestAction = possibleActions[0];
            let bestValue = qValues[bestAction];
            
            for (let i = 1; i < possibleActions.length; i++) {
                const action = possibleActions[i];
                if (qValues[action] > bestValue) {
                    bestValue = qValues[action];
                    bestAction = action;
                }
            }
            
            // Store current activations for visualization
            await getNetworkActivations(x, y);
            
            return bestAction;
        }
        
        // Get Q-values for a state
        async function getQValues(x, y) {
            const stateTensor = createStateRepresentation(x, y);
            
            // Get Q-values from network
            const qValues = await tf.tidy(() => {
                const predictions = qNetwork.predict(stateTensor.expandDims(0));
                return predictions.dataSync();
            });
            
            // Clean up tensors
            stateTensor.dispose();
            
            return qValues;
        }
        
        // Take a step in the environment
        function takeStep(x, y, action) {
            const { dx, dy } = ACTIONS[action];
            const newX = x + dx;
            const newY = y + dy;
            
            let reward = REWARDS.step; // Default step reward
            
            // Check if the action leads to a wall
            if (getCellType(newX, newY) === 'wall') {
                reward = REWARDS.wall;
                return { newX: x, newY: y, reward, done: false }; // Stay in place
            }
            
            // Check for goal or trap
            const cellType = getCellType(newX, newY);
            if (cellType === 'goal') {
                reward = REWARDS.goal;
                return { newX, newY, reward, done: true };
            } else if (cellType === 'trap') {
                reward = REWARDS.trap;
                return { newX, newY, reward, done: true };
            }
            
            // Add environment-specific mechanics
            if (environmentTypeSelect.value === 'ice' && Math.random() < 0.2) {
                // Slippery ice: 20% chance to move in a random direction
                const randomAction = Math.floor(Math.random() * actionCount);
                const randomMove = ACTIONS[randomAction];
                const slipX = x + randomMove.dx;
                const slipY = y + randomMove.dy;
                
                // Check if slipped into a wall
                if (getCellType(slipX, slipY) === 'wall') {
                    return { newX, newY, reward, done: false }; // Use the original move
                }
                
                return { newX: slipX, newY: slipY, reward, done: false };
            }
            
            return { newX, newY, reward, done: false };
        }
        
        // Add experience to replay buffer
        function addToReplayBuffer(state, action, reward, nextState, done) {
            // Add to buffer
            replayBuffer.push({
                state,
                action,
                reward,
                nextState,
                done
            });
            
            // Keep buffer size limited
            if (replayBuffer.length > replayBufferSize) {
                replayBuffer.shift(); // Remove oldest experience
            }
            
            // Update buffer size display
            bufferSizeDisplay.textContent = `${replayBuffer.length}/${replayBufferSize}`;
            
            // Update replay buffer visualization if needed
            if (showReplayBufferCheckbox.checked) {
                updateReplayBufferVisualization();
            }
        }
        
        // Sample batch from replay buffer
        function sampleFromReplayBuffer(batchSize) {
            // Ensure buffer has enough samples
            const sampleSize = Math.min(batchSize, replayBuffer.length);
            
            // Sample random indices
            const indices = [];
            for (let i = 0; i < sampleSize; i++) {
                indices.push(Math.floor(Math.random() * replayBuffer.length));
            }
            
            // Extract experiences
            return indices.map(idx => replayBuffer[idx]);
        }
        
        // Train network on a batch of experiences
        async function trainOnBatch() {
            // Check if buffer has enough samples
            if (replayBuffer.length < batchSize) {
                return 0; // Not enough samples, skip training
            }
            
            // Sample batch
            const batch = sampleFromReplayBuffer(batchSize);
            
            // Prepare tensors
            return tf.tidy(() => {
                // Extract states, actions, rewards, next states, and done flags
                const states = tf.concat(batch.map(exp => createStateRepresentation(exp.state.x, exp.state.y).expandDims(0)));
                const nextStates = tf.concat(batch.map(exp => createStateRepresentation(exp.nextState.x, exp.nextState.y).expandDims(0)));
                const actions = tf.tensor1d(batch.map(exp => exp.action), 'int32');
                const rewards = tf.tensor1d(batch.map(exp => exp.reward));
                const dones = tf.tensor1d(batch.map(exp => exp.done ? 0.0 : 1.0)); // 0 if done, 1 otherwise
                
                // Get current Q-values for all actions
                const currentQValues = qNetwork.predict(states);
                
                // Get target Q-values from target network
                const targetQValues = targetNetwork.predict(nextStates);
                
                // Compute max Q-values for next states
                const maxNextQ = targetQValues.max(1); // Max over actions
                
                // Compute target Q-values: rewards + gamma * max(Q(s',a')) * (1 - done)
                const targetQ = rewards.add(tf.mul(tf.mul(maxNextQ, tf.scalar(discountFactor)), dones));
                
                // Create target Q-value matrix by copying current Q-values
                const targetQMatrix = currentQValues.clone();
                
                // Update target Q-values for the actions taken
                const indices = tf.stack([tf.range(0, batch.length, 1), actions], 1);
                targetQMatrix.scatter(indices, targetQ);
                
                // Compute loss and optimize
                let loss = 0;
                optimizer.minimize(() => {
                    const predictions = qNetwork.predict(states);
                    const mse = tf.losses.meanSquaredError(targetQMatrix, predictions);
                    loss = mse.dataSync()[0];
                    return mse;
                });
                
                return loss;
            });
        }
        
        // Update epsilon value
        function updateEpsilon() {
            // Linear decay from epsilonStart to epsilonEnd over epsilonDecay steps
            const decay = Math.max(0, 1 - trainingSteps / epsilonDecay);
            currentEpsilon = epsilonEnd + (epsilonStart - epsilonEnd) * decay;
            
            // Update UI
            currentEpsilonDisplay.textContent = currentEpsilon.toFixed(2);
            
            return currentEpsilon;
        }
        
        // Get network activations for a given state (for visualization)
        async function getNetworkActivations(x, y) {
            // Create input state
            const stateTensor = createStateRepresentation(x, y);
            
            // Get activations from each layer
            const activations = [];
            
            await tf.tidy(() => {
                // Get input activations (one-hot encoded state)
                const inputActivations = stateTensor.arraySync();
                activations.push(inputActivations);
                
                // Forward pass through the network layer by layer
                let layerInput = stateTensor.expandDims(0);
                const layers = qNetwork.layers;
                
                for (let i = 0; i < layers.length; i++) {
                    const layer = layers[i];
                    layerInput = layer.apply(layerInput);
                    
                    // Extract activations
                    const layerActivations = layerInput.squeeze().arraySync();
                    activations.push(Array.isArray(layerActivations) ? layerActivations : [layerActivations]);
                }
            });
            
            // Clean up tensors
            stateTensor.dispose();
            
            // Store activations
            currentActivations = activations;
            
            // Update network visualization
            drawNetworkVisualization();
        }
        
        // Run complete DQN training
        async function runDQN() {
            if (isTraining) return;
            
            isTraining = true;
            stepByStepMode = false;
            currentEpisode = 0;
            episodeRewards = [];
            episodeSteps = [];
            losses = [];
            epsilonHistory = [];
            trainingSteps = 0;
            
            algorithmStatusDisplay.textContent = 'Training...';
            
            // Disable buttons during training
            setControlsEnabled(false);
            
            try {
                // Run episodes
                for (let episode = 0; episode < maxEpisodes; episode++) {
                    currentEpisode = episode + 1;
                    const episodeResult = await runEpisode();
                    
                    // Update tracking
                    episodeRewards.push(episodeResult.totalReward);
                    episodeSteps.push(episodeResult.steps);
                    
                    // Update UI
                    currentEpisodeDisplay.textContent = `${currentEpisode}/${maxEpisodes}`;
                    totalRewardDisplay.textContent = episodeResult.totalReward.toFixed(2);
                    currentStepDisplay.textContent = episodeResult.steps;
                    
                    // Update charts every 10 episodes
                    if (episode % 10 === 0 || episode === maxEpisodes - 1) {
                        updateCharts();
                        render();
                        
                        // Update model info
                        updateModelInfo();
                        
                        // Update network visualization
                        drawNetworkVisualization();
                        
                        // Small delay to allow UI updates
                        await new Promise(resolve => setTimeout(resolve, 1));
                    }
                }
                
                algorithmStatusDisplay.textContent = 'Training Complete';
            } catch (err) {
                console.error('Error during training:', err);
                algorithmStatusDisplay.textContent = 'Training Error';
            } finally {
                // Re-enable controls
                setControlsEnabled(true);
                isTraining = false;
                
                // Final UI updates
                updateCharts();
                render();
                updateModelInfo();
                drawNetworkVisualization();
            }
        }
        
        // Run one episode of DQN
        async function runEpisode() {
            // Reset agent position to start
            agentPosition = { ...startPosition };
            let episodeReward = 0;
            let done = false;
            let steps = 0;
            const maxSteps = gridSize * gridSize * 4; // Limit steps to avoid infinite loops
            
            while (!done && steps < maxSteps) {
                steps++;
                trainingSteps++;
                
                // Choose action using epsilon-greedy policy
                const state = { ...agentPosition };
                const action = await chooseAction(agentPosition.x, agentPosition.y);
                
                // Take the action
                const result = takeStep(agentPosition.x, agentPosition.y, action);
                
                // Update agent position
                agentPosition.x = result.newX;
                agentPosition.y = result.newY;
                
                // Add to replay buffer
                addToReplayBuffer(
                    state,
                    action,
                    result.reward,
                    { ...agentPosition },
                    result.done
                );
                
                // Train on a batch if buffer has enough samples
                if (replayBuffer.length >= batchSize) {
                    lastBatchLoss = await trainOnBatch();
                    losses.push({ step: trainingSteps, loss: lastBatchLoss });
                    currentLossDisplay.textContent = lastBatchLoss.toFixed(4);
                }
                
                // Update target network periodically
                if (trainingSteps % targetUpdateFrequency === 0) {
                    updateTargetNetwork();
                }
                
                // Update epsilon
                currentEpsilon = updateEpsilon();
                epsilonHistory.push({ step: trainingSteps, epsilon: currentEpsilon });
                
                // Accumulate reward
                episodeReward += result.reward;
                
                // Check if episode is done
                done = result.done;
                
                // Render occasionally for visual feedback during training
                if (steps % 100 === 0) {
                    render();
                    await new Promise(resolve => setTimeout(resolve, 1));
                }
            }
            
            return { totalReward: episodeReward, steps };
        }
        
        // Step through DQN manually
        async function stepDQN() {
            if (isTraining) return;
            
            stepByStepMode = true;
            isTraining = true;
            
            try {
                // If starting a new episode
                if (currentEpisode === 0 || isTerminalState(agentPosition.x, agentPosition.y)) {
                    currentEpisode++;
                    agentPosition = { ...startPosition };
                    totalReward = 0;
                    stepCount = 0;
                    
                    currentEpisodeDisplay.textContent = `${currentEpisode}/${maxEpisodes}`;
                    algorithmStatusDisplay.textContent = 'Episode Started';
                }
                
                // Take one step
                trainingSteps++;
                stepCount++;
                
                // Store original state
                const state = { ...agentPosition };
                
                // Choose action using epsilon-greedy policy
                const action = await chooseAction(agentPosition.x, agentPosition.y);
                
                // Take the action
                const result = takeStep(agentPosition.x, agentPosition.y, action);
                
                // Update agent position
                agentPosition.x = result.newX;
                agentPosition.y = result.newY;
                
                // Add to replay buffer
                addToReplayBuffer(
                    state,
                    action,
                    result.reward,
                    { ...agentPosition },
                    result.done
                );
                
                // Train on a batch if buffer has enough samples
                if (replayBuffer.length >= batchSize) {
                    lastBatchLoss = await trainOnBatch();
                    losses.push({ step: trainingSteps, loss: lastBatchLoss });
                    currentLossDisplay.textContent = lastBatchLoss.toFixed(4);
                }
                
                // Update target network periodically
                if (trainingSteps % targetUpdateFrequency === 0) {
                    updateTargetNetwork();
                    algorithmStatusDisplay.textContent = 'Target Network Updated';
                }
                
                // Update epsilon
                currentEpsilon = updateEpsilon();
                epsilonHistory.push({ step: trainingSteps, epsilon: currentEpsilon });
                
                // Update tracking
                totalReward += result.reward;
                
                // Update UI
                totalRewardDisplay.textContent = totalReward.toFixed(2);
                currentStepDisplay.textContent = stepCount;
                
                if (result.done) {
                    algorithmStatusDisplay.textContent = 'Episode Complete';
                    
                    // Record episode data
                    episodeRewards.push(totalReward);
                    episodeSteps.push(stepCount);
                    
                    // Update charts
                    updateCharts();
                } else {
                    algorithmStatusDisplay.textContent = 'Stepping...';
                }
                
                // Render
                render();
                
                // Update network visualization
                drawNetworkVisualization();
                
                // Update model info
                updateModelInfo();
            } catch (err) {
                console.error('Error during step:', err);
                algorithmStatusDisplay.textContent = 'Step Error';
            } finally {
                isTraining = false;
            }
        }
        
        // Run the agent using the learned policy
        async function runAgent() {
            if (isTraining || isRunningAgent) return;
            
            isRunningAgent = true;
            agentPosition = { ...startPosition };
            totalReward = 0;
            stepCount = 0;
            
            algorithmStatusDisplay.textContent = 'Agent Running...';
            
            // Disable controls during agent run
            setControlsEnabled(false);
            
            try {
                let done = false;
                const maxSteps = gridSize * gridSize * 4;
                
                while (!done && stepCount < maxSteps) {
                    // Get best action according to Q-network (no exploration)
                    const action = await getBestAction(agentPosition.x, agentPosition.y);
                    
                    // Take the action
                    const result = takeStep(agentPosition.x, agentPosition.y, action);
                    
                    // Update agent position
                    agentPosition.x = result.newX;
                    agentPosition.y = result.newY;
                    
                    // Update tracking
                    totalReward += result.reward;
                    stepCount++;
                    
                    // Update UI
                    totalRewardDisplay.textContent = totalReward.toFixed(2);
                    currentStepDisplay.textContent = stepCount;
                    
                    // Render
                    render();
                    
                    // Update network visualization
                    drawNetworkVisualization();
                    
                    // Delay based on animation speed
                    await new Promise(resolve => setTimeout(resolve, 100 - animationSpeed));
                    
                    // Check if episode is done
                    done = result.done;
                }
                
                algorithmStatusDisplay.textContent = done ? 'Agent Reached Goal' : 'Agent Max Steps';
            } catch (err) {
                console.error('Error during agent run:', err);
                algorithmStatusDisplay.textContent = 'Agent Run Error';
            } finally {
                // Re-enable controls
                setControlsEnabled(true);
                isRunningAgent = false;
            }
        }
        
        // Step the agent manually
        async function stepAgent() {
            if (isTraining || isRunningAgent) return;
            
            // Reset if in terminal state
            if (isTerminalState(agentPosition.x, agentPosition.y)) {
                agentPosition = { ...startPosition };
                totalReward = 0;
                stepCount = 0;
                
                totalRewardDisplay.textContent = totalReward.toFixed(2);
                currentStepDisplay.textContent = stepCount;
                algorithmStatusDisplay.textContent = 'Agent Reset';
                
                render();
                return;
            }
            
            // Get best action according to Q-network (no exploration)
            const action = await getBestAction(agentPosition.x, agentPosition.y);
            
            // Take the action
            const result = takeStep(agentPosition.x, agentPosition.y, action);
            
            // Update agent position
            agentPosition.x = result.newX;
            agentPosition.y = result.newY;
            
            // Update tracking
            totalReward += result.reward;
            stepCount++;
            
            // Update UI
            totalRewardDisplay.textContent = totalReward.toFixed(2);
            currentStepDisplay.textContent = stepCount;
            
            if (result.done) {
                algorithmStatusDisplay.textContent = 'Agent Reached Terminal State';
            } else {
                algorithmStatusDisplay.textContent = 'Agent Stepped';
            }
            
            // Render
            render();
            
            // Update network visualization
            drawNetworkVisualization();
        }
        
        // Update charts
        function updateCharts() {
            // Update learning chart
            if (learningChartInstance) {
                // Create labels for episodes
                const labels = Array.from({ length: episodeRewards.length }, (_, i) => i + 1);
                
                // Update learning chart data
                learningChartInstance.data.labels = labels;
                learningChartInstance.data.datasets[0].data = episodeRewards;
                learningChartInstance.data.datasets[1].data = episodeSteps;
                
                learningChartInstance.update();
            }
            
            // Update loss chart
            if (lossChartInstance && losses.length > 0 && epsilonHistory.length > 0) {
                // Sample steps for x-axis (to avoid too many points)
                const maxSteps = 100;
                const stepSize = Math.max(1, Math.floor(losses.length / maxSteps));
                
                // Sample loss values
                const sampledLosses = [];
                for (let i = 0; i < losses.length; i += stepSize) {
                    sampledLosses.push(losses[i]);
                }
                
                // Sample epsilon values (align with loss steps)
                const sampledEpsilons = [];
                for (let i = 0; i < epsilonHistory.length; i += stepSize) {
                    if (i < epsilonHistory.length) {
                        sampledEpsilons.push(epsilonHistory[i]);
                    }
                }
                
                // Prepare data for chart
                lossChartInstance.data.labels = sampledLosses.map(l => l.step);
                lossChartInstance.data.datasets[0].data = sampledLosses.map(l => l.loss);
                
                // Add epsilon data if available
                if (sampledEpsilons.length > 0) {
                    lossChartInstance.data.datasets[1].data = sampledEpsilons.map(e => e.epsilon);
                }
                
                lossChartInstance.update();
            }
        }
        
        // Update replay buffer visualization
        function updateReplayBufferVisualization() {
            if (!showReplayBufferCheckbox.checked) return;
            
            // Clear current samples
            replaySamplesDisplay.innerHTML = '';
            
            // Get the most recent samples (up to 5)
            const numSamples = Math.min(5, replayBuffer.length);
            const samples = replayBuffer.slice(-numSamples);
            
            // Create sample rows
            for (const sample of samples) {
                const row = document.createElement('div');
                row.className = 'grid grid-cols-4 gap-1 text-xs py-1 border-b border-gray-200 dark:border-gray-700';
                
                // State
                const stateCell = document.createElement('div');
                stateCell.textContent = `(${sample.state.x},${sample.state.y})`;
                row.appendChild(stateCell);
                
                // Action
                const actionCell = document.createElement('div');
                actionCell.textContent = ACTION_NAMES[sample.action];
                row.appendChild(actionCell);
                
                // Reward
                const rewardCell = document.createElement('div');
                rewardCell.textContent = sample.reward.toFixed(1);
                if (sample.reward > 0) {
                    rewardCell.classList.add('text-green-500');
                } else if (sample.reward < 0) {
                    rewardCell.classList.add('text-red-500');
                }
                row.appendChild(rewardCell);
                
                // Next State
                const nextStateCell = document.createElement('div');
                nextStateCell.textContent = `(${sample.nextState.x},${sample.nextState.y})`;
                if (sample.done) {
                    nextStateCell.classList.add('text-purple-500');
                }
                row.appendChild(nextStateCell);
                
                replaySamplesDisplay.appendChild(row);
            }
        }
        
        // Update model information display
        function updateModelInfo() {
            if (!qNetwork) return;
            
            // Model summary
            modelSummaryDisplay.textContent = 'DQN with layers:\n';
            
            // Add input layer info
            modelSummaryDisplay.textContent += `- Input: [${stateCount}] (One-hot state)\n`;
            
            // Add hidden layers
            for (let i = 0; i < networkLayerSizes.length; i++) {
                modelSummaryDisplay.textContent += `- Dense: ${networkLayerSizes[i]} (ReLU)\n`;
            }
            
            // Add output layer
            modelSummaryDisplay.textContent += `- Output: ${actionCount} (Linear)`;
            
            // Input shape
            inputShapeDisplay.textContent = `Grid size: ${gridSize}×${gridSize}, State encoding: One-hot (${stateCount} dimensions)`;
            
            // Output shape
            outputShapeDisplay.textContent = `Action space: ${actionCount} (${ACTION_NAMES.join(', ')})`;
            
            // Experience replay info
            experienceReplayInfoDisplay.textContent = `Buffer size: ${replayBuffer.length}/${replayBufferSize}, Batch size: ${batchSize}`;
            if (trainingSteps > 0) {
                experienceReplayInfoDisplay.textContent += `, Target updates: ${Math.floor(trainingSteps / targetUpdateFrequency)}`;
            }
        }
        
        // Draw neural network visualization
        function drawNetworkVisualization() {
            // Clear SVG
            networkSvg.innerHTML = '';
            
            // Check if network details should be shown
            if (!showNetworkDetailsCheckbox.checked) {
                // Just show a simple message
                const text = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                text.setAttribute('x', '50%');
                text.setAttribute('y', '50%');
                text.setAttribute('text-anchor', 'middle');
                text.setAttribute('dominant-baseline', 'middle');
                text.setAttribute('fill', document.body.classList.contains('dark') ? '#D1D5DB' : '#1F2937');
                text.textContent = 'Enable "Show Network Details" to see the neural network';
                networkSvg.appendChild(text);
                return;
            }
            
            // Get SVG dimensions
            const width = networkSvg.clientWidth;
            const height = networkSvg.clientHeight;
            
            // Define layer positions
            const layers = [stateCount, ...networkLayerSizes, actionCount];
            const layerSpacing = width / (layers.length + 1);
            const nodeSpacing = 30;
            
            // Calculate max neurons to show per layer (to avoid overcrowding)
            const maxNeuronsToShow = Math.floor(height / nodeSpacing);
            
            // Draw connections first (so they appear behind nodes)
            for (let l = 0; l < layers.length - 1; l++) {
                const x1 = (l + 1) * layerSpacing;
                const x2 = (l + 2) * layerSpacing;
                
                // Determine how many neurons to show in each layer
                const sourceNeurons = Math.min(layers[l], maxNeuronsToShow);
                const targetNeurons = Math.min(layers[l + 1], maxNeuronsToShow);
                
                const sourceSkip = layers[l] > maxNeuronsToShow ? Math.ceil(layers[l] / maxNeuronsToShow) : 1;
                const targetSkip = layers[l + 1] > maxNeuronsToShow ? Math.ceil(layers[l + 1] / maxNeuronsToShow) : 1;
                
                // Draw connections
                for (let i = 0; i < sourceNeurons; i++) {
                    // Calculate actual neuron index
                    const sourceIdx = Math.min(i * sourceSkip, layers[l] - 1);
                    const y1 = height / 2 - (sourceNeurons * nodeSpacing) / 2 + i * nodeSpacing;
                    
                    // Get neuron activation
                    const sourceActivation = l < currentActivations.length && sourceIdx < currentActivations[l].length 
                        ? currentActivations[l][sourceIdx] 
                        : 0;
                    
                    for (let j = 0; j < targetNeurons; j++) {
                        // Calculate actual neuron index
                        const targetIdx = Math.min(j * targetSkip, layers[l + 1] - 1);
                        const y2 = height / 2 - (targetNeurons * nodeSpacing) / 2 + j * nodeSpacing;
                        
                        // Get target neuron activation
                        const targetActivation = l + 1 < currentActivations.length && targetIdx < currentActivations[l + 1].length 
                            ? currentActivations[l + 1][targetIdx] 
                            : 0;
                        
                        // Create connection
                        const line = document.createElementNS('http://www.w3.org/2000/svg', 'line');
                        line.setAttribute('x1', x1);
                        line.setAttribute('y1', y1);
                        line.setAttribute('x2', x2);
                        line.setAttribute('y2', y2);
                        
                        // Set connection color based on activations
                        const connectionStrength = Math.abs(sourceActivation * targetActivation);
                        const isDarkMode = document.body.classList.contains('dark');
                        
                        if (connectionStrength > 0.5) {
                            // Active connection
                            line.setAttribute('stroke', COLORS.activeConnection.light);
                            line.setAttribute('stroke-width', 1 + connectionStrength);
                            line.setAttribute('opacity', 0.7);
                        } else {
                            // Inactive connection
                            line.setAttribute('stroke', isDarkMode ? COLORS.connection.dark : COLORS.connection.light);
                            line.setAttribute('stroke-width', 0.5);
                            line.setAttribute('opacity', 0.3);
                        }
                        
                        networkSvg.appendChild(line);
                    }
                }
            }
            
            // Draw nodes for each layer
            for (let l = 0; l < layers.length; l++) {
                const x = (l + 1) * layerSpacing;
                
                // Determine how many neurons to show
                const neuronsToShow = Math.min(layers[l], maxNeuronsToShow);
                const skip = layers[l] > maxNeuronsToShow ? Math.ceil(layers[l] / maxNeuronsToShow) : 1;
                
                // Add layer label
                const label = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                label.setAttribute('x', x);
                label.setAttribute('y', 20);
                label.setAttribute('text-anchor', 'middle');
                label.setAttribute('fill', document.body.classList.contains('dark') ? '#D1D5DB' : '#1F2937');
                label.setAttribute('font-size', '12');
                
                if (l === 0) {
                    label.textContent = 'Input';
                } else if (l === layers.length - 1) {
                    label.textContent = 'Output';
                } else {
                    label.textContent = `Hidden ${l}`;
                }
                
                networkSvg.appendChild(label);
                
                // Draw individual neurons
                for (let i = 0; i < neuronsToShow; i++) {
                    // Calculate actual neuron index
                    const neuronIdx = Math.min(i * skip, layers[l] - 1);
                    
                    const y = height / 2 - (neuronsToShow * nodeSpacing) / 2 + i * nodeSpacing;
                    
                    // Get neuron activation
                    const activation = l < currentActivations.length && neuronIdx < currentActivations[l].length 
                        ? currentActivations[l][neuronIdx] 
                        : 0;
                    
                    // Create neuron
                    const circle = document.createElementNS('http://www.w3.org/2000/svg', 'circle');
                    circle.setAttribute('cx', x);
                    circle.setAttribute('cy', y);
                    circle.setAttribute('r', 5);
                    
                    // Highlight active neurons
                    if (activation > 0.5) {
                        circle.setAttribute('fill', COLORS.activeNeuron.light);
                    } else {
                        circle.setAttribute('fill', COLORS.neuron.light);
                    }
                    
                    // Add stroke
                    circle.setAttribute('stroke', document.body.classList.contains('dark') ? '#F3F4F6' : '#1F2937');
                    circle.setAttribute('stroke-width', '1');
                    
                    networkSvg.appendChild(circle);
                    
                    // Add activation label for output layer
                    if (l === layers.length - 1) {
                        const valueLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                        valueLabel.setAttribute('x', x + 15);
                        valueLabel.setAttribute('y', y + 4);
                        valueLabel.setAttribute('fill', document.body.classList.contains('dark') ? '#D1D5DB' : '#1F2937');
                        valueLabel.setAttribute('font-size', '12');
                        valueLabel.textContent = ACTION_NAMES[neuronIdx] + `: ${activation.toFixed(2)}`;
                        networkSvg.appendChild(valueLabel);
                    }
                    
                    // Show dots for skipped neurons
                    if (i === neuronsToShow - 1 && neuronsToShow < layers[l]) {
                        const ellipsis = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                        ellipsis.setAttribute('x', x);
                        ellipsis.setAttribute('y', y + 20);
                        ellipsis.setAttribute('text-anchor', 'middle');
                        ellipsis.setAttribute('fill', document.body.classList.contains('dark') ? '#D1D5DB' : '#1F2937');
                        ellipsis.setAttribute('font-size', '16');
                        ellipsis.textContent = '⋮';
                        networkSvg.appendChild(ellipsis);
                        
                        // Add count of hidden neurons
                        const countLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                        countLabel.setAttribute('x', x);
                        countLabel.setAttribute('y', y + 35);
                        countLabel.setAttribute('text-anchor', 'middle');
                        countLabel.setAttribute('fill', document.body.classList.contains('dark') ? '#D1D5DB' : '#1F2937');
                        countLabel.setAttribute('font-size', '10');
                        countLabel.textContent = `(${layers[l]} total)`;
                        networkSvg.appendChild(countLabel);
                    }
                }
            }
        }
        
        // Enable or disable controls during processing
        function setControlsEnabled(enabled) {
            runDqnBtn.disabled = !enabled;
            stepDqnBtn.disabled = !enabled;
            resetDqnBtn.disabled = !enabled;
            runAgentBtn.disabled = !enabled;
            stepAgentBtn.disabled = !enabled;
            gridSizeSelect.disabled = !enabled;
            environmentTypeSelect.disabled = !enabled;
            resetEnvironmentBtn.disabled = !enabled;
            networkArchitectureSelect.disabled = !enabled;
        }
        
        // Render the grid world
        async function render() {
            const isDarkMode = document.body.classList.contains('dark');
            
            // Clear canvas
            ctx.clearRect(0, 0, gridWorldCanvas.width, gridWorldCanvas.height);
            
            // Draw grid cells
            for (let y = 0; y < gridSize; y++) {
                for (let x = 0; x < gridSize; x++) {
                    const cellType = environment[y][x];
                    
                    // Cell position
                    const cellX = x * cellSize;
                    const cellY = y * cellSize;
                    
                    // Set cell color based on type
                    let cellColor;
                    switch (cellType) {
                        case 'wall':
                            cellColor = isDarkMode ? COLORS.wall.dark : COLORS.wall.light;
                            break;
                        case 'goal':
                            cellColor = COLORS.goal.light;
                            break;
                        case 'trap':
                            cellColor = COLORS.trap.light;
                            break;
                        case 'start':
                            cellColor = isDarkMode ? COLORS.empty.dark : COLORS.empty.light;
                            break;
                        default:
                            cellColor = isDarkMode ? COLORS.empty.dark : COLORS.empty.light;
                    }
                    
                    // Fill cell with base color
                    ctx.fillStyle = cellColor;
                    ctx.fillRect(cellX, cellY, cellSize, cellSize);
                    
                    // Draw cell border
                    ctx.strokeStyle = isDarkMode ? COLORS.grid.dark : COLORS.grid.light;
                    ctx.lineWidth = 1;
                    ctx.strokeRect(cellX, cellY, cellSize, cellSize);
                    
                    // Draw special markers for start and goal
                    if (cellType === 'start') {
                        ctx.fillStyle = COLORS.start.light;
                        const padding = cellSize * 0.2;
                        ctx.beginPath();
                        ctx.arc(cellX + cellSize/2, cellY + cellSize/2, cellSize/2 - padding, 0, Math.PI * 2);
                        ctx.fill();
                        
                        // Draw 'S' in the start cell
                        ctx.fillStyle = isDarkMode ? COLORS.text.dark : COLORS.text.light;
                        ctx.font = `${cellSize * 0.6}px Arial`;
                        ctx.textAlign = 'center';
                        ctx.textBaseline = 'middle';
                        ctx.fillText('S', cellX + cellSize/2, cellY + cellSize/2);
                    } else if (cellType === 'goal') {
                        // Draw 'G' in the goal cell
                        ctx.fillStyle = isDarkMode ? COLORS.text.dark : COLORS.text.light;
                        ctx.font = `${cellSize * 0.6}px Arial`;
                        ctx.textAlign = 'center';
                        ctx.textBaseline = 'middle';
                        ctx.fillText('G', cellX + cellSize/2, cellY + cellSize/2);
                    } else if (cellType === 'trap') {
                        // Draw 'T' in the trap cell
                        ctx.fillStyle = isDarkMode ? COLORS.text.dark : COLORS.text.light;
                        ctx.font = `${cellSize * 0.6}px Arial`;
                        ctx.textAlign = 'center';
                        ctx.textBaseline = 'middle';
                        ctx.fillText('T', cellX + cellSize/2, cellY + cellSize/2);
                    }
                    
                    // Show Q-values if enabled (for non-wall cells) and if network is trained
                    if (showQValuesCheckbox.checked && cellType !== 'wall' && qNetwork) {
                        await drawQValuesForDQN(x, y, cellX, cellY);
                    }
                    
                    // Show policy arrows if enabled (for non-wall cells) and if network is trained
                    if (showPolicyCheckbox.checked && cellType !== 'wall' && !isTerminalState(x, y) && qNetwork) {
                        await drawPolicyArrowForDQN(x, y, cellX, cellY);
                    }
                }
            }
            
            // Draw agent
            const agentX = agentPosition.x * cellSize;
            const agentY = agentPosition.y * cellSize;
            
            ctx.fillStyle = COLORS.agent.light;
            const padding = cellSize * 0.15;
            ctx.fillRect(agentX + padding, agentY + padding, cellSize - 2*padding, cellSize - 2*padding);
            
            // Draw eyes to make it look like a character
            ctx.fillStyle = isDarkMode ? COLORS.text.dark : COLORS.text.light;
            const eyeSize = cellSize * 0.1;
            const eyeY = agentY + cellSize * 0.35;
            ctx.fillRect(agentX + cellSize * 0.3, eyeY, eyeSize, eyeSize);
            ctx.fillRect(agentX + cellSize * 0.6, eyeY, eyeSize, eyeSize);
            
            // Draw mouth
            ctx.beginPath();
            ctx.moveTo(agentX + cellSize * 0.3, agentY + cellSize * 0.65);
            ctx.lineTo(agentX + cellSize * 0.7, agentY + cellSize * 0.65);
            ctx.lineWidth = 2;
            ctx.stroke();
        }
        
        // Draw Q-values for DQN
        async function drawQValuesForDQN(x, y, cellX, cellY) {
            // Get Q-values from network
            const qValues = await getQValues(x, y);
            
            const fontSize = Math.max(8, Math.min(12, cellSize / 5));
            
            ctx.font = `${fontSize}px Arial`;
            ctx.textAlign = 'center';
            ctx.textBaseline = 'middle';
            
            // Draw Q-values for each action
            for (let action = 0; action < actionCount; action++) {
                const qValue = qValues[action];
                const { dx, dy } = ACTIONS[action];
                
                // Skip if action leads to wall
                if (getCellType(x + dx, y + dy) === 'wall') continue;
                
                // Position for Q-value text
                let textX, textY;
                
                switch (action) {
                    case 0: // Up
                        textX = cellX + cellSize / 2;
                        textY = cellY + cellSize * 0.2;
                        break;
                    case 1: // Right
                        textX = cellX + cellSize * 0.8;
                        textY = cellY + cellSize / 2;
                        break;
                    case 2: // Down
                        textX = cellX + cellSize / 2;
                        textY = cellY + cellSize * 0.8;
                        break;
                    case 3: // Left
                        textX = cellX + cellSize * 0.2;
                        textY = cellY + cellSize / 2;
                        break;
                }
                
                // Normalize Q-value for color (assuming values generally between -1 and +1)
                const normalizedQ = Math.max(0, Math.min(1, (qValue + 1) / 2));
                
                // Set color based on Q-value
                if (qValue >= 0) {
                    // Positive values: green to yellow
                    const g = Math.floor(255 - normalizedQ * 100);
                    ctx.fillStyle = `rgb(${Math.floor(normalizedQ * 255)}, ${g}, 0)`;
                } else {
                    // Negative values: red
                    const intensity = Math.floor(Math.abs(normalizedQ) * 255);
                    ctx.fillStyle = `rgb(${intensity}, 0, 0)`;
                }
                
                // Draw background circle for better visibility
                ctx.beginPath();
                ctx.arc(textX, textY, fontSize * 0.8, 0, Math.PI * 2);
                ctx.fillStyle = document.body.classList.contains('dark') ? 'rgba(0, 0, 0, 0.7)' : 'rgba(255, 255, 255, 0.7)';
                ctx.fill();
                
                // Draw Q-value text
                ctx.fillStyle = document.body.classList.contains('dark') ? '#FFFFFF' : '#000000';
                ctx.fillText(qValue.toFixed(1), textX, textY);
            }
        }
        
        // Draw policy arrow for DQN
        async function drawPolicyArrowForDQN(x, y, cellX, cellY) {
            // Get best action
            const bestAction = await getBestAction(x, y);
            if (bestAction === null) return;
            
            const { dx, dy } = ACTIONS[bestAction];
            
            // Skip if best action leads to wall (shouldn't happen with proper training)
            if (getCellType(x + dx, y + dy) === 'wall') return;
            
            const centerX = cellX + cellSize / 2;
            const centerY = cellY + cellSize / 2;
            
            // Get Q-values to determine arrow confidence
            const qValues = await getQValues(x, y);
            const qValue = qValues[bestAction];
            
            // Get second best Q-value for comparison
            let secondBestValue = -Infinity;
            for (let a = 0; a < actionCount; a++) {
                if (a !== bestAction && qValues[a] > secondBestValue) {
                    secondBestValue = qValues[a];
                }
            }
            
            // Normalize Q-value for arrow properties
            const confidence = Math.min(1, Math.max(0, (qValue - secondBestValue)));
            
            // Draw arrow
            ctx.beginPath();
            
            // Move to center of cell
            ctx.moveTo(centerX, centerY);
            
            // Move in direction of best action
            const arrowLength = cellSize * 0.3 * (0.5 + 0.5 * confidence);
            const endX = centerX + dx * arrowLength;
            const endY = centerY + dy * arrowLength;
            ctx.lineTo(endX, endY);
            
            // Arrow head
            const headSize = cellSize * 0.15;
            let angle;
            
            switch (bestAction) {
                case 0: angle = -Math.PI / 2; break; // Up
                case 1: angle = 0; break;           // Right
                case 2: angle = Math.PI / 2; break; // Down
                case 3: angle = Math.PI; break;     // Left
            }
            
            ctx.lineTo(
                endX - headSize * Math.cos(angle - Math.PI / 6),
                endY - headSize * Math.sin(angle - Math.PI / 6)
            );
            
            ctx.moveTo(endX, endY);
            
            ctx.lineTo(
                endX - headSize * Math.cos(angle + Math.PI / 6),
                endY - headSize * Math.sin(angle + Math.PI / 6)
            );
            
            // Set arrow properties
            ctx.strokeStyle = document.body.classList.contains('dark') ? COLORS.arrow.dark : COLORS.arrow.light;
            ctx.lineWidth = 2 * (0.5 + 0.5 * confidence);
            
            // Set opacity based on confidence
            ctx.globalAlpha = 0.3 + 0.7 * confidence;
            
            // Draw the arrow
            ctx.stroke();
            
            // Reset opacity
            ctx.globalAlpha = 1.0;
        }
        
        // Initialize the application
        initializeApp();
    </script>
</body>
</html>